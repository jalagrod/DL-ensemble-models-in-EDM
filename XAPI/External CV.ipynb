{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should be True\n",
    "print(torch.cuda.device_count())  # Should be > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import multiprocessing\n",
    "import json\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 06:11:13,197\tWARNING worker.py:1422 -- SIGTERM handler is not set because current thread is not the main thread.\n",
      "2025-03-06 06:11:15,120\tINFO worker.py:1743 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Fold 1 completed in 32.61 minutes!\n",
      "\n",
      " Fold 2 completed in 55.57 minutes!\n",
      "\n",
      " Fold 3 completed in 32.88 minutes!\n",
      "\n",
      " Fold 4 completed in 33.15 minutes!\n",
      "\n",
      " Fold 5 completed in 33.07 minutes!\n",
      "\n",
      " Fold 6 completed in 33.46 minutes!\n",
      "\n",
      " Fold 7 completed in 32.99 minutes!\n",
      "\n",
      " Fold 8 completed in 56.42 minutes!\n",
      "\n",
      " Fold 9 completed in 57.26 minutes!\n",
      "\n",
      " Fold 10 completed in 33.17 minutes!\n",
      "\n",
      " All experiments completed! Results saved in `final_test_results.csv`\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Load dataset\n",
    "FILENAME = \"XAPI\"\n",
    "DATA_PATH = \"xapi.csv\"\n",
    "TARGET = \"Class\"\n",
    "KFOLD = 10  # Number of folds for cross-validation\n",
    "MAX_THREADS = 1  # Number of threads to use for parallel experiments\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.columns = df.columns.str.replace(' ', '')  # Remove spaces in column names\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET]\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "gpu_available = 1 if torch.cuda.is_available() else 0\n",
    "\n",
    "# Create Stratified KFold (ensuring each test set is unique)\n",
    "skf = StratifiedKFold(n_splits=KFOLD, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Define function to train and evaluate each fold\n",
    "def run_experiment(fold, train_index, test_index):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Split dataset into training and test sets\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    df_train = X_train.copy()\n",
    "    df_train[TARGET] = y_train\n",
    "    df_test = X_test.copy()\n",
    "    df_test[TARGET] = y_test\n",
    "\n",
    "    # Define output directory for this fold\n",
    "    path = f\"GPU_{gpu_available}_{FILENAME}_DL_VALIDATION_FOLD{fold}\"\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)  # Remove previous results\n",
    "\n",
    "    # Create AutoGluon predictor\n",
    "    predictor = TabularPredictor(\n",
    "        label=TARGET,\n",
    "        path=path,\n",
    "        problem_type=\"multiclass\",\n",
    "    )\n",
    "\n",
    "    # Fit model\n",
    "    predictor.fit(\n",
    "        df_train,\n",
    "        num_bag_folds=10,  \n",
    "        verbosity=0,\n",
    "        num_gpus=1 if gpu_available else 0,\n",
    "        excluded_model_types=['RF', 'KNN', 'GBM', 'XGB', 'CAT', 'XT', 'LR'],\n",
    "        presets=\"best_quality\"\n",
    "    )\n",
    "\n",
    "    # Evaluate model\n",
    "    y_pred = predictor.predict(df_test.drop(columns=[TARGET]))\n",
    "    y_prob = predictor.predict_proba(df_test.drop(columns=[TARGET]))\n",
    "\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    test_prec = precision_score(y_test, y_pred, average='weighted')\n",
    "    test_rec = recall_score(y_test, y_pred, average='weighted')\n",
    "    test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    test_roc_auc = roc_auc_score(pd.get_dummies(y_test), y_prob, average='weighted', multi_class='ovr')\n",
    "\n",
    "    # Save results\n",
    "    result = {\n",
    "        'fold': fold,\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_precision': test_prec,\n",
    "        'test_recall': test_rec,\n",
    "        'test_f1': test_f1,\n",
    "        'test_roc_auc': test_roc_auc,\n",
    "    }\n",
    "\n",
    "    # Save results to CSV\n",
    "    result_df = pd.DataFrame([result])\n",
    "    result_df.to_csv(f\"test_results_fold{fold}.csv\", index=False)\n",
    "\n",
    "    print(f\"\\n Fold {fold} completed in {result['runtime_minutes']:.2f} minutes!\")\n",
    "\n",
    "# Run experiments in parallel\n",
    "with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), start=1):\n",
    "        executor.submit(run_experiment, fold, train_idx, test_idx)\n",
    "\n",
    "# Combine results from all folds\n",
    "all_results = []\n",
    "for fold in range(1, KFOLD + 1):\n",
    "    temp_df = pd.read_csv(f\"test_results_fold{fold}.csv\")\n",
    "    all_results.append(temp_df)\n",
    "\n",
    "final_results = pd.concat(all_results, ignore_index=True)\n",
    "final_results.to_csv(\"final_test_results.csv\", index=False)\n",
    "\n",
    "print(\"\\n All experiments completed! Results saved in `final_test_results.csv`\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JAL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>p-value</th>\n",
       "      <th>median_difference</th>\n",
       "      <th>mean_difference</th>\n",
       "      <th>std_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.317311</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.317311</td>\n",
       "      <td>0.3805</td>\n",
       "      <td>0.3805</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.317311</td>\n",
       "      <td>0.3693</td>\n",
       "      <td>0.3693</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.317311</td>\n",
       "      <td>0.3754</td>\n",
       "      <td>0.3754</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      metric   p-value  median_difference  mean_difference  std_difference\n",
       "0   accuracy  0.317311             0.2324           0.2324             0.0\n",
       "1  precision  0.317311             0.3805           0.3805             0.0\n",
       "2     recall  0.317311             0.3693           0.3693             0.0\n",
       "3         f1  0.317311             0.3754           0.3754             0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==========================\n",
    "# WILCOXON SIGNED-RANK TEST\n",
    "# ==========================\n",
    "\n",
    "from scipy.stats import wilcoxon\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_metrics_by_fold = {\n",
    "    'accuracy': 0.9548,\n",
    "    'precision': 0.9597,\n",
    "    'recall': 0.9492,\n",
    "    'f1': 0.9522,\n",
    "    'roc_auc': 0.9523\n",
    "}\n",
    "\n",
    "# Reference values from Yu et al.\n",
    "yu_et_al_metrics = {\n",
    "    'accuracy': 0.7646,\n",
    "    'precision': 0.6165,\n",
    "    'recall': 0.6277,\n",
    "    'f1': 0.6216,\n",
    "    'roc_auc': None  # Not reported\n",
    "}\n",
    "\n",
    "# Dictionary to store Wilcoxon test results\n",
    "wilcoxon_results = []\n",
    "\n",
    "# Perform Wilcoxon Signed-Rank Test for each metric\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "    if yu_et_al_metrics[metric] is not None:\n",
    "        # Compute paired differences\n",
    "        differences = df_metrics_by_fold[metric] - yu_et_al_metrics[metric]\n",
    "        # Compute Wilcoxon test\n",
    "        stat, p_value = wilcoxon(differences, method='approx')\n",
    "                \n",
    "        # Compute descriptive statistics\n",
    "        median_diff = np.median(differences)\n",
    "        mean_diff = np.mean(differences)\n",
    "        std_diff = np.std(differences)\n",
    "\n",
    "        # Store results\n",
    "        wilcoxon_results.append({\n",
    "            'metric': metric,\n",
    "            'p-value': p_value,\n",
    "            'median_difference': median_diff,\n",
    "            'mean_difference': mean_diff,\n",
    "            'std_difference': std_diff\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_wilcoxon = pd.DataFrame(wilcoxon_results)\n",
    "df_wilcoxon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
